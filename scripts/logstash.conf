input {
  file {
    path => ["/var/log/network.log"]
    #start_position => "beginning"
    type => "syslog"
    tags => [ "netsyslog" ]
  }
  file {
    path => ["/var/log/system.log"]
    #start_position => "beginning"
    type => "syslog"
    tags => [ "syssyslog" ]
  }
  redis {
    host => "172.16.206.245"
    data_type => "list"
    key => "logstash"
    codec => json
    type => "redis"
  }
  tcp {
    port => 5001
    type => "IIS"
    tags => [ "dev_iis" ]
  }
  tcp {
    port => 5002
    type => "IIS"
    tags => [ "test_iis" ]
  }
  tcp {
    port => 5003
    type => "IIS"
    tags => [ "stage_iis" ]
  }
  tcp {
    port => 5004
    type => "IIS"
    tags => [ "prod_iis" ]
  }
} #end input block

filter {
  if [type] == "syslog" {
    if [type] == "syslog" {
      grok {
        #strips timestamp and host off of the front of the syslog message leaving the raw message generated by the syslog client and saves it as "raw_message"
        patterns_dir => "/opt/logstash/patterns"
        match => [ "message", "%{TIMESTAMP_ISO8601:@timestamp} %{HOST:syslog_host} %{GREEDYDATA:raw_message}" ]
      }
    }

    #classify network syslog logs as palo alto or ASA or other log
    if "netsyslog" in [tags] {
      grep {
        drop => false
        match => [ "raw_message", "%ASA-" ]
        add_tag => [ "asa_log", "firewall" ]
      }
      grep {
        drop => false
        match => [ "raw_message", ",TRAFFIC," ]
        add_tag => [ "palo_alto_log", "firewall" ]
      }
      grep {
        drop => false
        match => [ "raw_message", ",THREAT," ]
        add_tag => [ "palo_alto_threat_log", "firewall" ]
      }
      grep {
        drop => false
        match => [ "raw_message", "User=.*Flags=.*cmd=" ]
        add_tag => [ "tacacs_accounting_log", "tacacs" ]
      }
      if "firewall" not in [tags] {
        mutate {
          add_tag => [ "generic_log" ]
        }
      }
    }

    if "palo_alto_log" in [tags] {
      #parse into csv and fix @timestamp to match the generate time of the log within the palo alto.
      csv {
        source => "raw_message"
        columns => [ "PaloAltoDomain","ReceiveTime","SerialNum","Type","Threat-ContentType","ConfigVersion","GenerateTime","SourceAddress","DestinationAddress","NATSourceIP","NATDestinationIP","Rule","SourceUser","DestinationUser","Application","VirtualSystem","SourceZone","DestinationZone","InboundInterface","OutboundInterface","LogAction","TimeLogged","SessionID","RepeatCount","SourcePort","DestinationPort","NATSourcePort","NATDestinationPort","Flags","IPProtocol","Action","Bytes","BytesSent","BytesReceived","Packets","StartTime","ElapsedTimeInSec","Category","Padding","seqno","actionflags","SourceCountry","DestinationCountry","cpadding","pkts_sent","pkts_received" ]
      }
      date {
        timezone => "America/New_York"
        match => [ "GenerateTime", "YYYY/MM/dd HH:mm:ss" ]
      }
      #convert fields to proper format
      mutate {
        convert => [ "Bytes", "integer" ]
        convert => [ "BytesReceived", "integer" ]
        convert => [ "BytesSent", "integer" ]
        convert => [ "ElapsedTimeInSec", "integer" ]
        convert => [ "geoip.area_code", "integer" ]
        convert => [ "geoip.dma_code", "integer" ]
        convert => [ "geoip.latitude", "float" ]
        convert => [ "geoip.longitude", "float" ]
        convert => [ "NATDestinationPort", "integer" ]
        convert => [ "NATSourcePort", "integer" ]
        convert => [ "Packets", "integer" ]
        convert => [ "pkts_received", "integer" ]
        convert => [ "pkts_sent", "integer" ]
        convert => [ "seqno", "integer" ]
        gsub => [ "Rule", " ", "_",
                  "Application", "( |-)", "_" ]
        remove_field => [ "message", "raw_message" ]
      }
    } else if "palo_alto_threat_log" in [tags] {
      #parse into csv and fix @timestamp to match the generate time of the log within the palo alto.
      csv {
        source => "raw_message"
        columns => [ "FUTURE_USE","ReceiveTime","SerialNum","Type","Subtype","FUTURE_USE","GenerateTime","SourceAddress","DestinationAddress","NATSourceIP","NATDestinationIP","Rule","SourceUser","DestinationUser","Application","VirtualSystem","SourceZone","DestinationZone","InboundInterface","OutboundInterface","LogAction","FUTURE_USE","SessionID","RepeatCount","SourcePort","DestinationPort","NATSourcePort","NATDestinationPort","Flags","IPProtocol","Action","Miscellaneous","ThreatID","Category","Severity","Direction","SequenceNumber","ActionFlags","SourceLocation","DestinationLocation","FUTURE_USE","ContentType" ]
      }
      date {
        timezone => "America/New_York"
        match => [ "GenerateTime", "YYYY/MM/dd HH:mm:ss" ]
      }
      #convert fields to proper format
      mutate {
        convert => [ "SessionID", "integer" ]
        convert => [ "SourcePort", "integer" ]
        convert => [ "DestinationPort", "integer" ]
        convert => [ "SerialNum", "integer" ]
        convert => [ "NATDestinationPort", "integer" ]
        convert => [ "NATSourcePort", "integer" ]
        convert => [ "SequenceNumber", "integer" ]
        convert => [ "RepeatCount", "integer" ]
        gsub => [ "Rule", " ", "_",
                  "Application", "( |-)", "_" ]
        remove_field => [ "message", "raw_message" ]
      }

    } else if "asa_log" in [tags] {
      #parse ASA log
      grok {
        patterns_dir => "/opt/logstash/patterns"
        break_on_match => false
        match => [ "raw_message", "%{CISCOFACSEVMNEM} %{WORD:Action} %{WORD:IPProtocol} src %{WORD:SourceZone}:%{IP:SourceAddress}\/%{POSINT:SourcePort} dst %{WORD:DestinationZone}:%{IP:DestinationAddress}\/%{POSINT:DestinationPort} by access-group \"%{NOTSPACE:rule}\"%{GREEDYDATA}",
                   "raw_message", "%{CISCOFACSEVMNEM} %{WORD:Action} %{IPPROTOCOL:IPProtocol} src %{WORD:SourceZone}:%{IP:SourceAddress} dst %{WORD:DestinationZone}:%{IP:DestinationAddress} %{DATA:icmp_type_code} by access-group \"%{WORD:Rule}\"%{GREEDYDATA}",
                   "raw_message", "%{CISCOFACSEVMNEM} %{GREEDYDATA:description}" ]
      }
      mutate {
        remove_field => [ "message", "raw_message" ]
        add_field => [ "Application", "asa_unknown" ]
        lowercase => [ "Action" ]
      }
    } else if "tacacs_accounting_log" in [tags] {
      grok {
        patterns_dir => "/opt/logstash/patterns"
        break_on_match => false
        match => [ "raw_message", "%{DATESTAMP} NAS_IP=%{IPORHOST:source_device} %{DATA} User=%{WORD:user} %{DATA} cmd=%{GREEDYDATA:command}" ]
      }
      mutate {
        remove_field => [ "raw_message" ]
      }
    } else {
      #apply actions to logs that don't match any particular type of log
    }

    #Geolocate logs that have SourceAddress and if that SourceAddress is a non-RFC1918 address
    if [SourceAddress] and [SourceAddress] !~ "(^127\.0\.0\.1)|(^10\.)|(^172\.1[6-9]\.)|(^172\.2[0-9]\.)|(^172\.3[0-1]\.)|(^192\.168\.)|(^169\.254\.)" {
        geoip {
             database => "/opt/logstash/vendor/geoip/GeoLiteCity.dat"
             source => "SourceAddress"
             target => "SourceGeo"
        }
        #Delete 0,0 in SourceGeo.location if equal to 0,0
        if ([SourceGeo.location] and [SourceGeo.location] =~ "0,0") {
          mutate {
            replace => [ "SourceGeo.location", "" ]
          }
        }
      }

    #Geolocate logs that have DestinationAddress and if that DestinationAddress is a non-RFC1918 address
    if [DestinationAddress] and [DestinationAddress] !~ "(^127\.0\.0\.1)|(^10\.)|(^172\.1[6-9]\.)|(^172\.2[0-9]\.)|(^172\.3[0-1]\.)|(^192\.168\.)|(^169\.254\.)" {
        geoip {
             database => "/opt/logstash/vendor/geoip/GeoLiteCity.dat"
             source => "DestinationAddress"
             target => "DestinationGeo"
        }
        #Delete 0,0 in DestinationGeo.location if equal to 0,0
        if ([DestinationGeo.location] and [DestinationGeo.location] =~ "0,0") {
          mutate {
            replace => [ "DestinationAddress.location", "" ]
          }
        }
      }

      #Takes the 5-tuple of source address, source port, destination address, destination port, and protocol and does a SHA1 hash to fingerprint the flow.  This is a useful
      #way to be able to do top N terms queries on flows, not just on one field.
      if "firewall" in [tags] and [SourceAddress] and [DestinationAddress] {
        fingerprint {
          concatenate_sources => true
          method => "SHA1"
          key => "logstash"
          source => [ "SourceAddress", "SourcePort", "DestinationAddress", "DestinationPort", "IPProtocol" ]
        }
      }



    #delete dst_coords if equal to 0,0
    if ([dst_coords] and [dst_coords] =~ "0,0") {
      mutate {
        remove_field => [ "dst_coords" ]
      }
    }
  } else if [type] == "IIS" {
     #ignore log comments
      if [message] =~ "^#" or "alive" in [message] {
        drop {}
      }

      grok {
        patterns_dir => "/opt/logstash/patterns"
        match => [ "message", '%{TIMESTAMP_ISO8601:log_timestamp} "%{NOTSPACE:iisSite}" "%{IPORHOST:ServerName}" %{IPORHOST:ServerIP} %{WORD:httpMethod} (?:-|%{URIPATH:uriStem}) (?:-|%{NOTSPACE:uriQuery}) %{NUMBER:servicePort:int} (?:-|%{NOTSPACE:username}) %{IPORHOST:clientHost} "%{NOTSPACE:httpVersion}" (?:-|\"%{DATA:userAgent}\") (?:-|\"%{DATA:cookie}\") (?:-|%{NOTSPACE:referer}) %{NOTSPACE:hostHeader} %{NUMBER:httpStatusCode:int} (?:-|%{NUMBER:httpSubStatusCode:int}) (?:-|%{NUMBER:windowsStatusCode:int}) (?:-|%{NUMBER:bytesSent:int}) (?:-|%{NUMBER:bytesReceived:int}) (?:-|%{NUMBER:timeTakenInMilliSecs:int}) (?:-|\"%{IPORHOST:xForwardedFor}\")' ]
      }
      
      #Set the Event Timesteamp from the log
       date {
          match => [ "log_timestamp", "YYYY-MM-dd HH:mm:ss", "YYYY-MM-dd HH:mm:ss.SSS" ]
        timezone => "Etc/UCT"
        } 
      
      #ruby { code => "event['kilobytes'] = event['bytes'] / 1024.0" }
      
      #https://logstash.jira.com/browse/LOGSTASH-1354
      #geoip{
      # source => "clienthost"
      #   add_tag => [ "geoip" ]
      #}

          #Geolocate logs that have clienthost and if that clienthost is a non-RFC1918 address
      if [clienthost] and [clienthost] !~ "(^127\.0\.0\.1)|(^10\.)|(^172\.1[6-9]\.)|(^172\.2[0-9]\.)|(^172\.3[0-1]\.)|(^192\.168\.)|(^169\.254\.)" {
          geoip {
               database => "/opt/logstash/vendor/geoip/GeoLiteCity.dat"
               source => "clienthost"
               target => "clienthostGeo"
          }
          #Delete 0,0 in SourceGeo.location if equal to 0,0
          if ([clienthostGeo.location] and [clienthostGeo.location] =~ "0,0") {
            mutate {
              replace => [ "clienthostGeo.location", "" ]
            }
          }
        }      
      
      useragent {
        source=> "userAgent"
        prefix=> "browser"
      }
      
      mutate {
          remove_field => [ "log_timestamp" ]
          #remove_field => [ "log_timestamp", "message", "userAgent" ]
          gsub => [ "browseros", " ", "_", 
                    "browseros_name", " ", "_" ]
        }
      } else if [type] == "redis" {
      #parse windows event logs shipped from windows servers
      if [type] == "Win32-EventLog" {
        grok {
          patterns_dir => "/opt/logstash/patterns"
          break_on_match => false
          match => [ "message", "%{DATA}(Logon Account:|Account Name:)%{SPACE}%{USERNAME:username}%{GREEDYDATA}",
                     "message", "%{DATA}%{GREEDYDATA}" ]
        }
        grok {
          patterns_dir => "/opt/logstash/patterns"
          break_on_match => false
          match => [ "message", "%{DATA}(Client Address:|Source Network Address:|Source Workstation:)%{SPACE}(::ffff:)?%{IPORHOST:client_address}%{GREEDYDATA}",
                     "message", "%{DATA}%{GREEDYDATA}" ]
        }
      }
    } else {

    }


} #end filter block

output {
  #stdout { codec => rubydebug }
  #stdout { debug => true }
  if [type] =="IIS" {
    elasticsearch {
      protocol => "node"
      node_name => "netman-logstash-iis"
      cluster => "jh-elasticsearch1"
      host => "jtaxes001.jtax.com"
      index => "iis-%{+YYYY.MM}"
    }
  } else {
    elasticsearch {
      protocol => "node"
      node_name => "netman-logstash"
      cluster => "jh-elasticsearch1"
      host => "jtaxes001.jtax.com"
    }
  }
} #end output block